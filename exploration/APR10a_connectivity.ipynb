{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92424dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import mne_connectivity\n",
    "import os\n",
    "import os.path as op\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pickle\n",
    "from warnings import filterwarnings\n",
    "from sys import argv\n",
    "import matplotlib.pyplot as plt\n",
    "from stormdb.access import Query\n",
    "import pandas as pd\n",
    "from src.decoding_functions import smooth_data\n",
    "import src.preprocessing as pfun\n",
    "filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Define relevant variables ################################\n",
    "# Project info\n",
    "project = 'MINDLAB2020_MEG-AuditoryPatternRecognition'\n",
    "project_dir = '/projects/' + project\n",
    "os.environ['MINDLABPROJ']= project\n",
    "os.environ['MNE_ROOT']='~/miniconda3/envs/mne'\n",
    "os.environ['MESA_GL_VERSION_OVERRIDE'] = '3.2'\n",
    "\n",
    "#Paths\n",
    "suffix = ''\n",
    "raw_path = project_dir + '/scratch/maxfiltered_data/tsss_st16_corr96'\n",
    "ica_path = project_dir + '/scratch/working_memory/ICA'\n",
    "avg_path = project_dir + '/scratch/working_memory/averages'\n",
    "log_path = project_dir + '/misc/working_memory_logs'\n",
    "\n",
    "subjects_dir = project_dir + '/scratch/fs_subjects_dir' # Free surfer subjects dir for parcellation and source localization\n",
    "fwd_path = project_dir + '/scratch/forward_models'\n",
    "\n",
    "# Subjects info:\n",
    "qy = Query(project)\n",
    "subs = qy.get_subjects()\n",
    "\n",
    "#Subject\n",
    "scode = 11\n",
    "# if len(argv) > 1:\n",
    "#     scode = int(argv[1])\n",
    "sub = subs[scode-1]\n",
    "\n",
    "print('output will be saved to the following filename:\\n\\n{}{}'.format(sub,suffix))\n",
    "\n",
    "# Create subject specific directories if they don't exist\n",
    "if not os.path.exists(avg_path + '/data/' + sub):\n",
    "    os.mkdir(avg_path + '/data/' + sub)\n",
    "if not os.path.exists(avg_path + '/figures/' + sub):\n",
    "    os.mkdir(avg_path + '/figures/' + sub)\n",
    "\n",
    "# Define output paths\n",
    "conn_path = avg_path + '/data/{}/{}_conn{}.p'.format(sub,sub,suffix)\n",
    "fig_path = avg_path + '/figures/{}/{}_conn{}.pdf'.format(sub,sub,suffix)\n",
    "\n",
    "# Define block names (original MEG names, new condition names and logfile names)\n",
    "conds_orig = ['main','inv'] # MEG block code\n",
    "conds = ['maintenance','manipulation'] # New block code\n",
    "lnames = ['recognize','invert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b13ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Epoch data #########################################\n",
    "\n",
    "# Epoching parameters\n",
    "reject = dict(mag = 4e-12, grad = 4000e-13) # rejection thresholds\n",
    "events_fun = pfun.main_task_events_fun # Event function (see src/preprocessing.py)\n",
    "tmin = -2 #epoch start\n",
    "tmax = 8 #epoch end\n",
    "l_freq = .05 #HP filter\n",
    "h_freq = None #LP filter\n",
    "baseline = -.2\n",
    "# Initialize\n",
    "epochs = {}\n",
    "print('\\n############### EPOCHING #################\\n')\n",
    "for cidx, c in enumerate(conds_orig):\n",
    "    nc = conds[cidx] # new condition name\n",
    "    \n",
    "    # Files to retrieve\n",
    "    fname = os.path.join(raw_path, sub, c + '_raw_tsss.fif')\n",
    "    icaname = os.path.join(ica_path, sub, c + '_raw_tsss-ica.fif')\n",
    "    lfname = op.join(log_path, sub[0:4] + '_' + lnames[cidx] + '_MEG.csv')\n",
    "    events_fun_kwargs = {'cond': nc,'lfname': lfname} # input to the events function (new condition name and logfile)\n",
    "               \n",
    "    #Epoching proper:\n",
    "    epochs[nc] = pfun.WM_epoching(data_path = fname, #raw data path\n",
    "                                  ica_path = icaname, #ICA components path\n",
    "                                  tmin = tmin, tmax = tmax, #Epoch times\n",
    "                                  l_freq = l_freq, h_freq = None, #Filterning options\n",
    "                                  resample = 100, bads = [], #Resample and bad channels to reject\n",
    "                                  baseline = None, notch_filter = 50, # Demean baseline\n",
    "                                  events_fun = events_fun, #Event function to use for epoching\n",
    "                                  events_fun_kwargs = events_fun_kwargs, #Arguments for event function\n",
    "                                  reject=reject) # thresholds to reject artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536866e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine epochs\n",
    "epochs = mne.concatenate_epochs([epochs[e] for e in epochs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dca61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Source localization\n",
    "# Get sensor data covariance\n",
    "data_cov = mne.compute_covariance(epochs.load_data().copy().pick_types('mag'),\n",
    "                                       tmin= 0, tmax = 6.25,rank ='info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth_tstep = 0.025\n",
    "# smooth_twin = 0.08\n",
    "# if smooth_tstep:\n",
    "#     new_data, new_times = smooth_data(epochs.get_data(), tstart=epochs.times[0],\n",
    "#                                       tstep=smooth_tstep, twin=smooth_twin,\n",
    "#                                       Fs=epochs.info['sfreq'], taxis=2)\n",
    "\n",
    "# new_info = epochs.info.copy()\n",
    "# new_info['sfreq'] = 1/smooth_tstep\n",
    "# epochs = mne.EpochsArray(new_data, info = new_info, events = epochs.events,\n",
    "#                          event_id = epochs.event_id,tmin = epochs.tmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Compute sources propper\n",
    "print('\\n computing sources \\n')\n",
    "fwd_fn = op.join(fwd_path, sub + '_vol-fwd.fif')\n",
    "fwd = mne.read_forward_solution(fwd_fn)\n",
    "#compute noise covariance\n",
    "# noise_cov = mne.compute_covariance(epochs,tmin = -1,\n",
    "#                                    tmax=0, rank='info')\n",
    "#     data_cov = mne.compute_covariance(epochs.load_data().copy().pick_types('mag'),\n",
    "#                                        tmin= 0, tmax = 6.25,rank ='info')\n",
    "## mne solution\n",
    "inv = mne.beamformer.make_lcmv(epochs['manip'].info,fwd,data_cov, reg=0.05,\n",
    "                                pick_ori='max-power', #noise_cov=noise_cov,#,depth = 0.95,\n",
    "                                weight_norm= 'nai', rank = 'info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply inverse solution\n",
    "src_epochs = mne.beamformer.apply_lcmv_epochs(epochs,inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d54be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load parcellation for specific subject\n",
    "label_file = subjects_dir + '/{}/mri/aparc.a2009s+aseg.mgz'.format(sub)\n",
    "labels = mne.get_volume_labels_from_aseg(label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455b1aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read subjec-specific source space\n",
    "src = mne.read_source_spaces(subjects_dir + '/{}/bem/{}_vol-src.fif'.format(sub,sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label time courses (see attlas for label names)\n",
    "clabels = ['ctx_rh_G_temp_sup-G_T_transv',\n",
    "           'Right-Thalamus-Proper',\n",
    "           'ctx_rh_G_and_S_cingul-Mid-Post',\n",
    "           'Right-Hippocampus',    \n",
    "           'ctx_rh_G_precuneus',\n",
    "           'ctx_lh_G_temp_sup-G_T_transv',\n",
    "           'Left-Thalamus-Proper',\n",
    "           'ctx_lh_G_and_S_cingul-Mid-Post',\n",
    "           'Left-Hippocampus',\n",
    "           'ctx_lh_G_precuneus'\n",
    "           ]\n",
    "\n",
    "stc_labels = []\n",
    "for cidx, c in enumerate(src_epochs):\n",
    "    print('extracting sources for epoch {}'.format(cidx+1))\n",
    "    stc_labels += [src_epochs[cidx].extract_label_time_course(labels = [label_file,clabels], src = src, mode = 'auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a477b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert list of ROI time courses to array\n",
    "roi_data = np.array(stc_labels)\n",
    "roi_data.shape\n",
    "## We may want to get rid of 0-lag corelations (use with care, it may get rid of signal):\n",
    "roi_data = mne_connectivity.symmetric_orth(roi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63906121",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross-correlation connectivity for different time periods\n",
    "periods = {'baseline': [-2,0], 'listen': [0,2],'transition': [1,3], 'imagine': [2,4]}\n",
    "Xcorr = {}\n",
    "for p in periods:\n",
    "    print('Calculating Xcorr for period ',p,' ', periods[p])\n",
    "    tix = [a and b for a,b in zip(src_epochs[0].times>=periods[p][0],src_epochs[0].times < periods[p][1])]\n",
    "    ctimes = src_epochs[0].times[tix]\n",
    "    print(ctimes)\n",
    "    # Initialize output array with shape nTrials * nROIs * nROIs * nTimeLags\n",
    "    Xcorr[p] = np.zeros((roi_data.shape[0],roi_data.shape[1],roi_data.shape[1],ctimes.shape[0]*2-1))\n",
    "    Xcorr[p][Xcorr[p]==0] = np.nan\n",
    "    # Loop over trials and pairs of regions\n",
    "    for t in range(roi_data.shape[0]):\n",
    "        print('Xcorr trial ', t+1)\n",
    "        for r1 in range(roi_data.shape[1]):\n",
    "            for r2 in range(roi_data.shape[1]):\n",
    "                if r2 > r1: # if not computed before\n",
    "                    ## Compute cross-correlation proper\n",
    "                    a = roi_data[t,r1,tix]\n",
    "                    b = roi_data[t,r2,tix]\n",
    "                    # Standarize data to get pearson's r output\n",
    "                    aa = (a - a.mean()) / (np.std(a) * len(a)) \n",
    "                    bb =  (b - b.mean()) / np.std(b)\n",
    "                    Xcorr[p][t,r1,r2,:] = scipy.signal(aa,bb,mode='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f954b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some plots\n",
    "for r1 in range(roi_data.shape[1]):\n",
    "    for r2 in range(roi_data.shape[1]):\n",
    "        if r2 > r1:         \n",
    "            plt.figure()\n",
    "            plt.plot(np.squeeze(Xcorr['listen'][:,r1,r2,:].mean(axis=0)))\n",
    "            plt.plot(np.squeeze(Xcorr['imagine'][:,r1,r2,:].mean(axis=0)))\n",
    "            #plt.plot(np.squeeze((Xcorr['listen'][:,r1,r2,:]-Xcorr['imagine'][:,r1,r2,:]).mean(axis=0)))\n",
    "            plt.title(str(r1) + ' '+ str(r2))\n",
    "            #plt.plot(np.squeeze(Xcorr['imagine'][:,r1,r2,:].mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfreq = epochs.info['sfreq']\n",
    "fmin = {'delta': .5, 'theta': 4}\n",
    "fmax = {'delta': 2, 'theta': 8}\n",
    "cwt_bands = {'delta': np.array([.5,.75,1,1.25,1.5,1.75,2]), 'theta': np.array([4,5,6,7,8])}\n",
    "cwt_n_cycles = {'delta': np.array([1,2,2,2,2,2,2]), 'theta': np.array([3,3,3,3,3])}\n",
    "periods = {'whole': [0,10]}#,'listen': [0,1.75], 'imagine': [2,4]}\n",
    "conn = {}\n",
    "for b in cwt_bands:\n",
    "    conn[b] = {}\n",
    "    for p in periods:\n",
    "        conn[b][p] = mne_connectivity.phase_slope_index(\n",
    "            roi_data, names=clabels, mode='cwt_morlet', cwt_freqs = cwt_bands[b], #method='pli',\n",
    "            cwt_n_cycles = cwt_n_cycles[b], sfreq=sfreq, fmin=fmin[b], fmax=fmax[b], #faverage=True,\n",
    "            n_jobs=1,tmin = periods[p][0], tmax = periods[p][1])#c mt_adaptive=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240461d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfile = open(conn_path,'wb')\n",
    "pickle.dump(conn,cfile)\n",
    "cfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39269de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,10,figsize = (30,8),sharex=True, sharey=True)\n",
    "for bix,b in enumerate(conn):\n",
    "    print(b)\n",
    "    bdata =  np.squeeze(conn[b]['whole'].get_data()).reshape(10,10,-1)\n",
    "    for rix, r in enumerate(conn[b]['whole'].names):\n",
    "        cdata = np.squeeze(bdata[rix,:,:].copy())\n",
    "        cdata[(rix+1):,:] = bdata[(rix+1):,rix,:]*-1\n",
    "        axx, axy = rix % 10, rix // 10 + 1*bix\n",
    "        ix = np.arange(10) + rix*10        \n",
    "        im = ax[axy, axx].imshow(cdata, aspect='auto',vmin=-.1,vmax=.1,\n",
    "                  interpolation='nearest',cmap='RdBu_r',extent=[epochs.times[0],epochs.times[-1],len(clabels),0])#origin='lower'\n",
    "#         if axy != 3:\n",
    "#             ax[axy, axx].set_xticks([])\n",
    "        if axy == 0:\n",
    "            ax[axy, axx].set_title(r)\n",
    "        \n",
    "        if axx == 0:\n",
    "            ax[axy, axx].set_yticks(np.arange(len(clabels)) + .5)\n",
    "            ax[axy, axx].set_yticklabels(clabels)\n",
    "fig.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cf6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Example Cross correlation functions\n",
    "# def xcorr(x, y):\n",
    "#     # x and y should be normalized\n",
    "#     # Divide by lem of x to obtain normalized values \n",
    "#     corr = signal.correlate(x / x.shape[0], y, mode=\"full\")\n",
    "#     lags = signal.correlation_lags(len(x), len(y), mode=\"full\")\n",
    "#     return corr, lags\n",
    "\n",
    "# def xcorr_window(W):\n",
    "#     wcorr = np.full((W.shape[0], W.shape[0], W.shape[1]*2-1), np.nan)\n",
    "#     for y in range(W.shape[0]):\n",
    "#         for x in range(W.shape[0]):\n",
    "#             if x >= y:\n",
    "#                 wcorr[x, y,:], lags = xcorr(W[x,:],W[y,:])\n",
    "#     return wcorr, lags\n",
    "\n",
    "# def xcorr_sliding(T, srate, wmin, wmax, wstep, times, tstart, tend):\n",
    "#     startix = np.argmin(abs(times-tstart))\n",
    "#     endix = np.argmin(abs(times-tend))\n",
    "#     sstep = np.round(srate * wstep)\n",
    "#     smin = np.round(srate * wmin)\n",
    "#     smax = np.round(srate * wmax)\n",
    "#     wcenters = np.arange(startix + smin, endix-smax+1, sstep)\n",
    "#     scorr = []\n",
    "#     out_times = []\n",
    "#     for wc in wcenters:\n",
    "#         out_times += [times[int(wc)]]\n",
    "#         wix = np.arange(wc-smin,wc+smax+1,1,dtype=int)\n",
    "#         #Normalize for values between -1 and 1\n",
    "#         wnorm = (T[:,wix] - T[:,wix].mean(axis=1,keepdims=True)) / T[:,wix].std(axis=1,keepdims=True)\n",
    "#         wcorr, lags = xcorr_window(wnorm)\n",
    "#         scorr += [wcorr.copy()]\n",
    "#     scorr = np.array(scorr)\n",
    "#     out_times = np.array(out_times)\n",
    "#     return scorr, lags / srate, out_times\n",
    "\n",
    "# def xcorr_trials(D,srate, wmin, wmax, wstep, times, tstart, tend, orth=False):\n",
    "#     tcorr = []\n",
    "#     if orth:\n",
    "#         print(\"orthogonalizing\")\n",
    "#         D = mne_connectivity.symmetric_orth(D)\n",
    "#     for d in range(D.shape[0]):\n",
    "#         print('processing epoch ', d + 1, ' / ', D.shape[0])\n",
    "#         scorr, lags, out_times = xcorr_sliding(D[d], srate, wmin, wmax, wstep,times, tstart,tend)\n",
    "#         tcorr += [scorr.copy()]\n",
    "#     tcorr = np.array(tcorr)\n",
    "#     return tcorr, lags, out_times\n",
    "\n",
    "# tcorr, lags, out_times = xcorr_trials(roi_data,srate=100, wmin = .3,wmax=.3, wstep = .02,\n",
    "#                                      times = src_epochs[0].times, tstart = 0, tend = 4, orth=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
